{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101cf426-a84f-420d-9953-1d86e0b6b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "from torch.nn import ModuleList, ModuleDict\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric_temporal.nn.hetero import HeteroGCLSTM\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "from airpollution_trf_graph_loader import AirpollutionDatasetLoader\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604e8314-2659-4852-a88e-aa9c2cfca010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers= 1\n",
    "_T= 12 # time horizon \n",
    "_city= 'madrid'\n",
    "_include_trf= True # include or not traffic data as input\n",
    "_synth_data = True #train using synthetic pollution/traffic data\n",
    "\n",
    "device_ =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff723d-b89c-4d54-8086-7168591e5e31",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f831f79-f4ce-4909-8dc4-b742ccdb2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trf': 4, 'ap0': 2, 'ap1': 5, 'ap2': 2, 'ap3': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader= AirpollutionDatasetLoader(_city, include_trf=_include_trf, synth=_synth_data)\n",
    "dataset=loader.get_dataset(T=_T)\n",
    "feature_dim= loader.get_feature_dim()\n",
    "feature_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfda6a6b-17aa-426d-9b54-e71325408cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CO', 'SO2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names_trf= loader.get_column_names('ap0')\n",
    "column_names_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2d678-686a-4623-8d7a-f213541b3b07",
   "metadata": {},
   "source": [
    "## Define GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e804d4-079e-4896-890c-ade41b3af7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels_dict, out_channels, metadata, nlayers=2):\n",
    "        super(HeteroGNN, self).__init__()      \n",
    "        self.linears= ModuleDict({v:torch.nn.Linear(128,i) for v,i in in_channels_dict.items()})\n",
    "        self.n_conv_layers=nlayers\n",
    "\n",
    "        self.convs=ModuleList()\n",
    "        self.convs.append(HeteroGCLSTM(in_channels_dict=in_channels_dict, out_channels=128, metadata=metadata))\n",
    "        \n",
    "        new_in_channel_dict={v:128 for v,i in in_channels_dict.items()}\n",
    "        for l in range(0,self.n_conv_layers-1):\n",
    "            self.convs.append(HeteroGCLSTM(in_channels_dict=new_in_channel_dict, out_channels=128, metadata=metadata))\n",
    "        \n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, h_dict_lst, c_dict_lst):\n",
    "        new_h_lst=[]\n",
    "        new_c_lst=[]\n",
    "        x= x_dict\n",
    "        for i in range(0,self.n_conv_layers):\n",
    "            h, c= self.convs[i](x, edge_index_dict)\n",
    "            x = {key: val.relu() for key, val in h.items()}\n",
    "            new_h_lst.append(x)\n",
    "            new_c_lst.append(c)\n",
    "        \n",
    "        h= {v: self.linears[v](emb_) for v,emb_ in x.items()}\n",
    "        new_h_lst.append(h)\n",
    "        return new_h_lst, new_c_lst\n",
    "\n",
    "embedding_dim=1\n",
    "model = HeteroGNN(in_channels_dict=feature_dim, out_channels= embedding_dim, metadata=dataset[0].metadata(), nlayers=n_layers)\n",
    "model = model.to(device_)\n",
    "    \n",
    "train_dataset, test_dataset = temporal_signal_split(dataset,  train_ratio=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d65374-5fac-4415-8d0d-8ae437ca46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8326 926 649 277\n"
     ]
    }
   ],
   "source": [
    "num_train_snapshots= sum(1 for _ in train_dataset)\n",
    "num_testeval_snapshots= sum(1 for _ in test_dataset)\n",
    "num_eval_snapshots = int(num_testeval_snapshots * 0.3)\n",
    "num_test_snapshots= num_testeval_snapshots - num_eval_snapshots\n",
    "print(num_train_snapshots, num_testeval_snapshots, num_test_snapshots, num_eval_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5b026-c77a-407a-9652-bee67fd85765",
   "metadata": {},
   "source": [
    "## Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27758f2c-ef31-4486-83e2-0852c682494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f046a5ef97914b729b944f8e4837a899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs...:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - MSE (train): 3811.70751953125  - MSE (test): 1709.2576904296875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - MSE (train): 1976.092041015625  - MSE (test): 1005.1232299804688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - MSE (train): 1356.797119140625  - MSE (test): 821.9926147460938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - MSE (train): 1240.0567626953125  - MSE (test): 806.9190063476562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - MSE (train): 1228.2769775390625  - MSE (test): 803.3260498046875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - MSE (train): 1221.73486328125  - MSE (test): 798.6423950195312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - MSE (train): 1215.0546875  - MSE (test): 796.3241577148438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - MSE (train): 1210.526123046875  - MSE (test): 794.477294921875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - MSE (train): 1207.159423828125  - MSE (test): 793.1341552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377d28519dc84e4ab9abec73e3a8a84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train snapshots...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training parameters\n",
    "n_epochs=600\n",
    "batch_size= 24 * 1 #hours (1-day batch)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def calculate_loss(y_hat_dict, y_dict):\n",
    "    loss_= 0\n",
    "\n",
    "    for p in y_hat_dict.keys():\n",
    "        if p != 'trf':\n",
    "            y_hat= y_hat_dict[p]\n",
    "            y_hat= torch.nan_to_num(y_hat)\n",
    "            particle_loss = torch.mean((y_hat-y_dict[p])**2) #MSE\n",
    "            loss_ += particle_loss\n",
    "    return loss_\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "loss_epochs=[]\n",
    "for epoch in tqdm(range(n_epochs), desc='Training epochs...'):\n",
    "    \n",
    "    batch_loss = 0\n",
    "    counter=1    \n",
    "    \n",
    "    train_epoch_cost =0\n",
    "    eval_epoch_cost= 0\n",
    "    h_lst=[None for i in range(0,n_layers)]\n",
    "    c_lst=[None for i in range(0,n_layers)]\n",
    "        \n",
    "    #train\n",
    "    for time, train_snapshot in tqdm(enumerate(train_dataset), desc='Train snapshots...', leave=False):\n",
    "\n",
    "        h_lst, c_lst = model(train_snapshot.x_dict, train_snapshot.edge_index_dict, h_lst, c_lst)\n",
    "\n",
    "        h_dict= h_lst[-1]\n",
    "\n",
    "        snap_train_loss= calculate_loss(h_dict, train_snapshot.y_dict)\n",
    "        \n",
    "        train_epoch_cost = train_epoch_cost + snap_train_loss  \n",
    "        batch_loss = batch_loss + snap_train_loss\n",
    "        \n",
    "        if counter == batch_size:\n",
    "            batch_loss = batch_loss / batch_size\n",
    "            batch_loss.backward(retain_graph=True)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            counter=1\n",
    "            batch_loss= 0\n",
    "            \n",
    "            h_lst=[None for i in range(0,n_layers)]\n",
    "            c_lst=[None for i in range(0,n_layers)]\n",
    "            \n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "    train_epoch_cost = train_epoch_cost / (time+1)\n",
    "    \n",
    "    # validation \n",
    "    with torch.no_grad(): \n",
    "        eval_h_lst=[None for i in range(0,n_layers)]\n",
    "        eval_c_lst=[None for i in range(0,n_layers)]\n",
    "        for time, test_snapshot in tqdm(enumerate(test_dataset), desc='Test snapshots...', leave=False):\n",
    "            if time < num_test_snapshots:\n",
    "                eval_h_lst, eval_c_lst = model(test_snapshot.x_dict, test_snapshot.edge_index_dict, eval_h_lst, eval_c_lst) \n",
    "                snap_eval_loss= calculate_loss(eval_h_lst[-1], test_snapshot.y_dict)\n",
    "                eval_epoch_cost = eval_epoch_cost + snap_eval_loss\n",
    "            \n",
    "    eval_epoch_cost = eval_epoch_cost / (time+1)\n",
    "   \n",
    "    if early_stopper.early_stop(eval_epoch_cost):             \n",
    "        print(f'EARLY STOP  AT epoch {epoch} - MSE (train): {train_epoch_cost}  - MSE (test): {eval_epoch_cost}')\n",
    "        break\n",
    "    print(f'Epoch {epoch} - MSE (train): {train_epoch_cost}  - MSE (test): {eval_epoch_cost}')\n",
    "    loss_epochs.append((train_epoch_cost.item(), eval_epoch_cost.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1099f74-8f1f-4e2e-9b82-a9d256d3882e",
   "metadata": {},
   "source": [
    "At this point the model has been trained. Let's compute the validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbc701-1d13-4e02-93c2-675f99aab772",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epochs_df= pd.DataFrame(loss_epochs, columns='train test'.split())\n",
    "loss_epochs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc8464-40a2-4a7f-8bd1-0d16eebf3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epochs_df.plot(grid=True, xlabel='epoch', ylabel='MSE');\n",
    "if not os.path.exists(os.path.join('figs')):\n",
    "    os.makedirs(os.path.join('figs'))\n",
    "\n",
    "synth_data_str=\"\"\n",
    "if _synth_data:\n",
    "    synth_data_str=\"synth\"\n",
    "    \n",
    "if _include_trf:\n",
    "    plt.savefig(os.path.join('figs',f'mse_loss_evol_{_city}_{_T}_trf_{synth_data_str}.png'), bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(os.path.join('figs',f'mse_loss_evol_{_city}_{_T}_no_trf_{synth_data_str}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd327c0-afe7-4e5e-911f-f4b0ce1511d5",
   "metadata": {},
   "source": [
    "## Get validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b28c4f-ce55-402c-9a44-a0a95ec2005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation \n",
    "with torch.no_grad(): \n",
    "    eval_h_lst=[None for i in range(0,n_layers)]\n",
    "    eval_c_lst=[None for i in range(0,n_layers)]\n",
    "    y_hat_dict={}\n",
    "    y_true_dict={}\n",
    "    for time, test_snapshot in tqdm(enumerate(test_dataset), desc='Eval snapshots...', leave=False):\n",
    "        if time >= num_test_snapshots:\n",
    "            eval_h_lst, eval_c_lst = model(test_snapshot.x_dict, test_snapshot.edge_index_dict, eval_h_lst, eval_c_lst) \n",
    "             #print(eval_h_lst[-1]['trf'])\n",
    "\n",
    "            for k in test_snapshot.y_dict.keys():\n",
    "                v= y_hat_dict.get(k,[])\n",
    "                v.append(eval_h_lst[-1][k])\n",
    "                y_hat_dict[k]= v\n",
    "                \n",
    "                v= y_true_dict.get(k,[])\n",
    "                v.append(test_snapshot.x_dict[k])\n",
    "                y_true_dict[k]= v\n",
    "\n",
    "    y_hat_df={}\n",
    "    for k,v in y_hat_dict.items():\n",
    "        _cols= loader.get_column_names(k)\n",
    "        y_hat_df[k]= pd.DataFrame(torch.vstack(v), columns=_cols)\n",
    "\n",
    "    y_true_df={}\n",
    "    for k,v in y_true_dict.items():\n",
    "        _cols= loader.get_column_names(k)\n",
    "        y_true_df[k]= pd.DataFrame(torch.vstack(v), columns=_cols)\n",
    "    \n",
    "    if not os.path.exists(os.path.join('results')):\n",
    "        os.makedirs(os.path.join('results'))\n",
    "\n",
    "    for k, _df in y_hat_df.items():\n",
    "        if _include_trf:\n",
    "            _df.to_csv(os.path.join('results',f'y_hat_{_city}_{_T}_{k}_trf_{synth_data_str}.csv'))\n",
    "        else:\n",
    "            _df.to_csv(os.path.join('results',f'y_hat_{_city}_{_T}_{k}_no_trf_{synth_data_str}.csv'))\n",
    "    for k, _df in y_true_df.items():\n",
    "        if _include_trf:\n",
    "            _df.to_csv(os.path.join('results',f'y_true_{_city}_{_T}_{k}_trf_{synth_data_str}.csv'))    \n",
    "        else:\n",
    "            _df.to_csv(os.path.join('results',f'y_true_{_city}_{_T}_{k}_no_trf_{synth_data_str}.csv'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae337b4f-8d21-4ac5-8893-95dfd4930179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"That's all folks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
